{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import wandb\n",
    "from utilities import *\n",
    "from config import *\n",
    "from dataloading import *\n",
    "from tqdm import tqdm\n",
    "from transformer import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmidataur\u001b[0m (\u001b[33mknot-theory\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Logging in...\")\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): BigramLanguageModel(\n",
      "    (token_embedding_table): Embedding(16, 384)\n",
      "    (position_embedding): Embedding(6, 384)\n",
      "    (sa_heads): MultiHeadAttention(\n",
      "      (heads): ModuleList(\n",
      "        (0-5): 6 x Head(\n",
      "          (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "          (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "          (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (blocks): Sequential(\n",
      "      (0): Block(\n",
      "        (sa): MultiHeadAttention(\n",
      "          (heads): ModuleList(\n",
      "            (0-5): 6 x Head(\n",
      "              (key): Linear(in_features=384, out_features=64, bias=False)\n",
      "              (query): Linear(in_features=384, out_features=64, bias=False)\n",
      "              (value): Linear(in_features=384, out_features=64, bias=False)\n",
      "              (dropout): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ffwd): FeedForward(\n",
      "          (net): Sequential(\n",
      "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "            (1): ReLU()\n",
      "            (2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "            (3): Dropout(p=0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=384, out_features=1, bias=True)\n",
      "    (output): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# assumes you're using the transformer\n",
    "# if you're using the MLP, you'll need to change the data pipeline and the final dimension\n",
    "# also you can modify the transformer config in the transformer.py file\n",
    "\n",
    "# setup the model\n",
    "model = BigramLanguageModel()\n",
    "\n",
    "# cuda? (gpu)\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda:0\"\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "\n",
    "# send to gpu (maybe)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "# load the model\n",
    "filename = PATH + \"/model/\" + MODELNAME + \".pth\"\n",
    "if os.path.isfile(filename):\n",
    "    model.load_state_dict(torch.load(filename, map_location=torch.device(device)))\n",
    "else:\n",
    "   raise Exception(\"Model not found\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using renderer: notebook_connected\n"
     ]
    }
   ],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-62b3aa4d-a84a\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, Hello } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-62b3aa4d-a84a\",\n",
       "      Hello,\n",
       "      {\"name\": \"World\"}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x2919cdad0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import circuitsvis as cv\n",
    "# Testing that the library works\n",
    "cv.examples.hello(\"World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix, HookedTransformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x2d38fa2d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['embed.W_E', 'pos_embed.W_pos', 'module.sa_heads.heads.0.key.weight', 'module.sa_heads.heads.0.query.weight', 'module.sa_heads.heads.0.value.weight', 'module.sa_heads.heads.1.key.weight', 'module.sa_heads.heads.1.query.weight', 'module.sa_heads.heads.1.value.weight', 'module.sa_heads.heads.2.key.weight', 'module.sa_heads.heads.2.query.weight', 'module.sa_heads.heads.2.value.weight', 'module.sa_heads.heads.3.key.weight', 'module.sa_heads.heads.3.query.weight', 'module.sa_heads.heads.3.value.weight', 'module.sa_heads.heads.4.key.weight', 'module.sa_heads.heads.4.query.weight', 'module.sa_heads.heads.4.value.weight', 'module.sa_heads.heads.5.key.weight', 'module.sa_heads.heads.5.query.weight', 'module.sa_heads.heads.5.value.weight', 'module.sa_heads.proj.weight', 'module.sa_heads.proj.bias', 'module.blocks.0.sa.heads.0.key.weight', 'module.blocks.0.sa.heads.0.query.weight', 'module.blocks.0.sa.heads.0.value.weight', 'module.blocks.0.sa.heads.1.key.weight', 'module.blocks.0.sa.heads.1.query.weight', 'module.blocks.0.sa.heads.1.value.weight', 'module.blocks.0.sa.heads.2.key.weight', 'module.blocks.0.sa.heads.2.query.weight', 'module.blocks.0.sa.heads.2.value.weight', 'module.blocks.0.sa.heads.3.key.weight', 'module.blocks.0.sa.heads.3.query.weight', 'module.blocks.0.sa.heads.3.value.weight', 'module.blocks.0.sa.heads.4.key.weight', 'module.blocks.0.sa.heads.4.query.weight', 'module.blocks.0.sa.heads.4.value.weight', 'module.blocks.0.sa.heads.5.key.weight', 'module.blocks.0.sa.heads.5.query.weight', 'module.blocks.0.sa.heads.5.value.weight', 'module.blocks.0.sa.proj.weight', 'module.blocks.0.sa.proj.bias', 'module.blocks.0.ffwd.net.0.weight', 'module.blocks.0.ffwd.net.0.bias', 'module.blocks.0.ffwd.net.2.weight', 'module.blocks.0.ffwd.net.2.bias', 'module.blocks.0.ln1.weight', 'module.blocks.0.ln1.bias', 'module.blocks.0.ln2.weight', 'module.blocks.0.ln2.bias', 'module.blocks.1.weight', 'module.blocks.1.bias', 'unembed.W_U', 'unembed.b_U'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import re\n",
    "\n",
    "# have to convert the state dict to the HookedTransformer format\n",
    "def translate(key, value):\n",
    "    direct_swaps = {\n",
    "        \"module.token_embedding_table.weight\": \"embed.W_E\",\n",
    "        \"module.position_embedding.weight\": \"pos_embed.W_pos\",\n",
    "        \"module.lm_head.weight\": \"unembed.W_U\",\n",
    "        \"module.lm_head.bias\": \"unembed.b_U\",\n",
    "    }\n",
    "\n",
    "    key = direct_swaps[key] if key in direct_swaps.keys() else key\n",
    "\n",
    "    # need to reshape the unembedding matrix\n",
    "    if key == \"unembed.W_U\":\n",
    "        value = value.reshape(-1, 1)\n",
    "\n",
    "    # attention heads\n",
    "    matches = re.match(r\"/module\\.sa_heads\\.heads\\.(\\d)\\.(.+)\", key)\n",
    "    if matches:\n",
    "        block = matches.group(1)\n",
    "        subkey = matches.group(2)\n",
    "\n",
    "        subkey_swaps = {\n",
    "            \"key.weight\": \"W_K\",\n",
    "            \"query.weight\": \"W_Q\",\n",
    "            \"value.weight\": \"W_V\",\n",
    "            \n",
    "        }\n",
    "\n",
    "    return (key, value)\n",
    "\n",
    "state_dict = torch.load(filename, map_location=torch.device(device))\n",
    "state_dict.keys()\n",
    "state_dict = OrderedDict(translate(k, v) for k, v in state_dict.items())\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([384, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict[\"unembed.W_U\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.0.mlp.W_in\n",
      "WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.0.mlp.W_out\n",
      "WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.0.attn.W_Q\n",
      "WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.0.attn.W_K\n",
      "WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.0.attn.W_O\n",
      "WARNING:root:Missing key for a weight matrix in pretrained, filled in with an empty tensor: blocks.0.attn.W_V\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wowza torch.Size([384, 1]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    n_layers=n_blocks,\n",
    "    d_model=n_embed,\n",
    "    d_head=n_embed // n_head,\n",
    "    n_ctx=MAX_LENGTH,\n",
    "    act_fn=\"relu\",\n",
    "    d_vocab=vocab_size,\n",
    "    d_vocab_out=1,\n",
    ")\n",
    "\n",
    "hooked = HookedTransformer(cfg)\n",
    "\n",
    "hooked.load_and_process_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataParallel' object has no attribute 'run_with_cache'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m logits, cache \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrun_with_cache([\u001b[39m12\u001b[39m,\u001b[39m4\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m8\u001b[39m,\u001b[39m7\u001b[39m,\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataParallel' object has no attribute 'run_with_cache'"
     ]
    }
   ],
   "source": [
    "logits, cache = model.run_with_cache([12,4,1,8,7,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
